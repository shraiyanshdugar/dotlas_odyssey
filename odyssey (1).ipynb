{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a70fb4f4-a35c-46a2-8388-ff1fbc466139",
          "showTitle": false,
          "title": ""
        },
        "id": "zChzt-4EmiEY"
      },
      "source": [
        "# Dotlas Odyssey 🏞 [40 points]\n",
        "\n",
        "> Data Engineering Assignment\n",
        "\n",
        "> `v1.0` Updated: Sep 2 2023\n",
        "\n",
        "Greetings Traveller,\n",
        "\n",
        "You have embarked on a mighty Odyssey, a journey filled with challenges, mysteries, and opportunities for growth. Your mission, should you choose to accept it, will involve navigating uncharted territories, solving complex problems, and contributing to a mission that will redefine our future. We believe that with your skills, passion, and determination, you can contribute significantly to this mission.\n",
        "\n",
        "In this notebook, you will find a series of assignments designed to test your skills, stretch your capabilities, and ultimately, prepare you for the journey ahead. Each task is a stepping stone towards your ultimate goal, and completing them will not only bring you closer to becoming a part of the Dotlas family but also to making a meaningful impact on the world.\n",
        "\n",
        "Remember, the journey of a thousand miles begins with a single step. So, gear up, stay positive, and embrace the adventure that awaits you.\n",
        "\n",
        "---\n",
        "<img src=\"https://dotlas-website.s3.eu-west-1.amazonaws.com/images/github/banner.png\" width=\"750px\" alt=\"dotlas\">\n",
        "\n",
        "> You are welcome to make your solution for the Dotlas Odyssey public as open-source and add it to your portfolio of projects, but only after all candidates have completed the assignment, or the position has been filled. Sharing your solution publicly while the assignment process is ongoing will result in disqualification from the selection process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "185e6bb9-9782-4ba4-8f7c-1d69385f7c6b",
          "showTitle": false,
          "title": ""
        },
        "id": "3KZo9eAxmiEd"
      },
      "source": [
        "## Tools & Technologies 🪛\n",
        "\n",
        "![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)\n",
        "![Anaconda](https://img.shields.io/badge/Anaconda-%2344A833.svg?style=for-the-badge&logo=anaconda&logoColor=white)\n",
        "![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)\n",
        "![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)\n",
        "![Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C.svg?style=for-the-badge&logo=Apache-Spark&logoColor=white)\n",
        "![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)\n",
        "![Numpy](https://img.shields.io/badge/NumPy-013243.svg?style=for-the-badge&logo=NumPy&logoColor=white)\n",
        "![Plotly](https://img.shields.io/badge/Plotly-%233F4F75.svg?style=for-the-badge&logo=plotly&logoColor=white)\n",
        "![AWS](https://img.shields.io/badge/AWS-%23FF9900.svg?style=for-the-badge&logo=amazon-aws&logoColor=white)\n",
        "\n",
        "- This exercise will be carried out using the [Python](https://www.python.org/) programming language and will rely hevily on the [Pandas](https://pandas.pydata.org/) library for data manipulation.\n",
        "- You are also free to use [Polars](https://www.pola.rs/), [Dask](https://www.dask.org/) or [Spark](https://spark.apache.org/docs/latest/api/python/index.html) if you do not want to use Pandas.\n",
        "- You may use any of [Matplotlib](https://matplotlib.org/), [Seaborn](https://seaborn.pydata.org/) or [Plotly](https://plotly.com/python/) packages for data visualization.\n",
        "- We will be using [Jupyter notebooks](https://jupyter.org/) to run Python code in order to view and interact better with our data and visualizations.\n",
        "- You are free to use [Google Colab](https://colab.research.google.com/) which provides an easy-to-use Jupyter interface.\n",
        "- When not in Colab, it is recommended to run this Jupyter Notebook within an [Anaconda](https://continuum.io/) environment\n",
        "- You can use any other Python packages that you deem fit for this project.\n",
        "\n",
        "> ⚠ **Ensure that your Python version is 3.9 or higher**\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/1/1b/Blue_Python_3.9_Shield_Badge.svg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3a1cd966-d7cc-45d9-b6be-a73e14d97f28",
          "showTitle": false,
          "title": ""
        },
        "id": "JR1ufEXsmiEf"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import ast\n",
        "import csv\n",
        "import requests\n",
        "import time\n",
        "import threading\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f93a1493-4f91-44eb-8fea-a58dd9862ed5",
          "showTitle": false,
          "title": ""
        },
        "id": "TF6t_HlfmiEg"
      },
      "source": [
        "## Section 1: Extract 🚰 [5]\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/8rEB2xzZcZDnBegHFS/giphy.gif\" width=\"250px\" alt=\"extract\">\n",
        "\n",
        "You have to extract 4 datasets. Each dataset contains partial information about **restaurants in the San Francisco Bay Area, California, US**. The datasets are as follows:\n",
        "1. **Delta**: A pipe delimited csv file that contains restaurants that offer food-delivery\n",
        "2. **Oscar**: A raw binary file (pickled) that contains restaurants that take reservations, and other high-end places\n",
        "3. **Tango**: A json file that contains all kinds of restaurants\n",
        "2. **Yankee**: A parquet file that contains crowdsourced restaurant data\n",
        "\n",
        "You also have one additional dataset called **Lookup Record** that contains the pre-defined mapping of restaurants between various rows of Delta through Yankee. Each row in the Lookup table can be considered as a single restaurant and can be assigned to one or more sources based on whether or not they exist for that restaurant. The Lookup records may have imperfect matches between sources, but you can ignore this quality issue and instead consider it as a source of truth for the amalgamation stage.\n",
        "\n",
        "\n",
        "### License 📜\n",
        "\n",
        "The data in this assignment is provided by Dotlas and is available for non-commercial use, such as research previews, and open-source projects. For any commercial use of the data, you must request access and provide all necessary context to [Dotlas](https://www.dotlas.com). Unauthorized commercial use of the data is prohibited. For more details, refer to the `DATA_LICENSE` file in the GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "05fd1db4-98d8-4bd6-991e-8a9b5ad7e3db",
          "showTitle": false,
          "title": ""
        },
        "id": "kzDGew_OmiEh"
      },
      "outputs": [],
      "source": [
        "delta_url: str = (\n",
        "    \"https://dotlas-marketing.s3.amazonaws.com/interviews/california_delta_dataset.csv\"\n",
        ")\n",
        "oscar_url: str = (\n",
        "    \"https://dotlas-marketing.s3.amazonaws.com/interviews/california_oscar_dataset.pkl\"\n",
        ")\n",
        "tango_url: str = (\n",
        "    \"https://dotlas-marketing.s3.amazonaws.com/interviews/california_tango_dataset.json\"\n",
        ")\n",
        "yankee_url: str = (\n",
        "    \"https://dotlas-marketing.s3.amazonaws.com/interviews/california_yankee_dataset.parquet\"\n",
        ")\n",
        "\n",
        "lookup_record: str = (\n",
        "    \"https://dotlas-marketing.s3.amazonaws.com/interviews/california_master_record.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "27433a9e-42d7-431e-91ef-f986f85acb4f",
          "showTitle": false,
          "title": ""
        },
        "id": "M-zgjsDomiEi"
      },
      "source": [
        "grf4egrjbknhhnddsd    hnk# 1.1 Read Data 🔀 [2]\n",
        "\n",
        "Load each source data file into separate DataFrames. Do not download the files locally to the file system but instead read them directly in-memory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "def read_files_into_dataframes(file_path):\n",
        "  df=pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "  try:\n",
        "            # Determine the file format based on the file extension\n",
        "      if file_path.endswith('.csv'):\n",
        "          df = pd.read_csv(file_path)\n",
        "      elif file_path.endswith('.parquet'):\n",
        "          df = pd.read_parquet(file_path)\n",
        "      elif file_path.endswith('.pkl'):\n",
        "          df = pd.read_pickle(file_path)\n",
        "      elif file_path.endswith('.json'):\n",
        "          df=pd.read_json(file_path)\n",
        "      else:\n",
        "          raise ValueError(f\"Unsupported file format: {file_path}\")\n",
        "\n",
        "            # Store the DataFrame in the dictionary with the specified name\n",
        "\n",
        "\n",
        "      print(f\"Successfully read  \")\n",
        "\n",
        "  except Exception as e:\n",
        "            # Handle exceptions\n",
        "      print(f\"Error reading : {str(e)}\")\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Access the DataFrames using their names\n",
        "\n",
        "table_c = read_files_into_dataframes(yankee_url)\n",
        "table_a = read_files_into_dataframes(oscar_url)\n",
        "table_b = read_files_into_dataframes(tango_url)\n",
        "table_d  = pd.read_csv(delta_url , delimiter='|')\n",
        "table_e = read_files_into_dataframes(lookup_record)"
      ],
      "metadata": {
        "id": "J3WKCdLGm85J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## functions\n",
        "1. replace_city\\\n",
        "    for table_c we have abnormality of address so we handled here\n",
        "2. handling_address\\\n",
        "    for table_a we have abnomality of addree so we handled here\n",
        "3. remove_html_tags\\\n",
        "    for removing html tags from text\n",
        "4. handling_html_tags\\\n",
        "    using above function(removing_html_tags) to remove tags from columns\n",
        "5. handling_space\\\n",
        "    convert space with None\n",
        "6. has_html_tags\\\n",
        "    to  check wether our column have html tags\n",
        "7. keep_unique_value\\\n",
        "    in our list only keep unique value after removing extra space or upper/lower case case\n",
        "9. check_url\\\n",
        "    to check wether url in right format\n",
        "10. check_url_change_none\\\n",
        "      put None to those urls which are not in right format\n",
        "11. website_url_formatting\\\n",
        "      format proper to http or https and remove extra space\n",
        "12. check_number\\\n",
        "      to check wether our number are in correct format of (ddd) ddd-dddd\n",
        "13. fromat_phone_number\\\n",
        "      format our phone number to (ddd) ddd-dddd format\n",
        "14. add_www\\\n",
        "      some website might be usable if we just add www. at starting (special case )\n",
        "15. clean_and_format_facebook \\\n",
        "      to clean our facebook urls.\n",
        "16. *convert_operation: \\\n",
        "      its special function for table_a operating_hours \\\n",
        "      where it convert our text to list according to monday-sunday \\\n",
        "      and put value on them in format of \"9:00 AM to 5:00 PM\" \\\n",
        "      contain multiple internal function for handling data \\\n",
        "      1.for extracting days \\\n",
        "      2.for extracting times \\\n",
        "      3.mapping days form 0-7 \\\n",
        "\n",
        "      it will only return if our both extracted day format and time match else it will return None\n",
        "17. union_and_process_lists \\\n",
        "      it will merge two list and keep unique of them\n",
        "18. find_columns_with_empty_values \\\n",
        "      to get us all rows which conatin space or if list column then tell if any list have none in it and tell if any our column contain None in it\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u2yzl78EE_Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def replace_city(row):\n",
        "  if row['address']:\n",
        "    if row['city'] in row['address']:\n",
        "        return row['address'].replace(row['city'], f\", {row['city']}\")\n",
        "  return row[\"address\"]\n",
        "def handling_address(x):\n",
        "  if x is not None:\n",
        "    x=x.replace(', ,','@').replace(' CA,','').replace(',',' , CA',1).replace(\"@\",\" , \" )\n",
        "  return x\n",
        "def remove_html_tags(text):\n",
        "    if text is None:\n",
        "        return None\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    return soup.get_text()\n",
        "def handling_html_tags(df,column):\n",
        "  df[column]=df[column].apply(remove_html_tags)\n",
        "  df[column]=df[column].apply(lambda value : None if value == '' else value)\n",
        "\n",
        "\n",
        "def handling_space(df,column):\n",
        "  df[column]=df[column].apply(lambda value : None if value == \"\" else value )\n",
        "\n",
        "def handling_space_operation(df,column):\n",
        "  df[column]=df[column].apply(lambda value : None if value == \"\" else value )\n",
        "\n",
        "\n",
        "def has_html_tags(text):\n",
        "    if text is None:\n",
        "        return False\n",
        "\n",
        "    # Regular expression to match HTML tags\n",
        "    html_tag_pattern = re.compile(r'<.*?>')\n",
        "\n",
        "    # Search for HTML tags in the text\n",
        "    return bool(html_tag_pattern.search(text))\n",
        "\n",
        "def keep_unique_values(strings):\n",
        "    cleaned_strings = {}\n",
        "    result = []\n",
        "\n",
        "    for string in strings:\n",
        "        # Clean the string by removing extra spaces and making it lowercase\n",
        "        cleaned_string = ' '.join(string.split()).lower()\n",
        "\n",
        "        # Check if the cleaned string is already in the dictionary\n",
        "        if cleaned_string not in cleaned_strings:\n",
        "            # If not, add it to the dictionary and the result list\n",
        "            cleaned_strings[cleaned_string] = True\n",
        "            result.append(string)\n",
        "\n",
        "    return result\n",
        "\n",
        "def check_url(df,column):\n",
        "  filtered_urls=pd.DataFrame()\n",
        "  url_pattern = r'^(https?://)[^\\s/$.?#].[^\\s]*$'\n",
        "  filtered_urls = df[df[column].notna()]\n",
        "  filtered_urls = filtered_urls[~filtered_urls[column].str.contains(url_pattern)]\n",
        "  print(filtered_urls.shape)\n",
        "  print(filtered_urls[column].head(2))\n",
        "\n",
        "def check_url_change_none(df,column):\n",
        "  filtered_urls=pd.DataFrame()\n",
        "  url_pattern = r'^(https?://)[^\\s/$.?#].[^\\s]*$'\n",
        "  filtered_urls = df[df[column].notna()]\n",
        "  filtered_urls = filtered_urls[~filtered_urls[column].str.contains(url_pattern)]\n",
        "  print(filtered_urls.shape)\n",
        "  index=filtered_urls.index\n",
        "  for i, index_value in enumerate(filtered_urls.index):\n",
        "    df.at[index_value,column] = None\n",
        "\n",
        "\n",
        "def website_url_fomatting(url):\n",
        "    # Convert the URL to lowercase\n",
        "    if url is None:\n",
        "      return None\n",
        "    url = url.lower()\n",
        "\n",
        "    # Check if the URL starts with \"http://\" or \"https://\"\n",
        "    if url.startswith(\"http://\"):\n",
        "        # Replace \"http://\" with \"https://www.\"\n",
        "        url = url.split(\"www.\", 1)[-1]\n",
        "        url = \"https://www.\" + url[len(\"http://\"):]\n",
        "    elif url.startswith(\"https://\"):\n",
        "        # Replace \"https://\" with \"https://www.\"\n",
        "        url = url.split(\"www.\", 1)[-1]\n",
        "        url = \"https://www.\" + url[len(\"https://\"):]\n",
        "\n",
        "    # Remove the specific substring \"it will be available soon\"\n",
        "\n",
        "    return url\n",
        "\n",
        "def check_number(df,column):\n",
        "  pattern = r'^\\(\\d{3}\\) \\d{3}-\\d{4}$'\n",
        "  all_in_desired_format = df[column].str.match(pattern, na=False).all()\n",
        "  print(all_in_desired_format)\n",
        "def format_phone_number(phone):\n",
        "    if phone is None:\n",
        "        return None\n",
        "    phone=phone.replace(\"+1\",\"\")\n",
        "    phone = ''.join(filter(str.isdigit, phone))  # Remove non-digit characters\n",
        "    if len(phone) == 10:  # Check if it's a valid phone number\n",
        "        return f'({phone[:3]}) {phone[3:6]}-{phone[6:]}'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def add_www(url):\n",
        "    if url is not None and not url.startswith('www.') and not url.startswith('http'):\n",
        "        return 'www.' + url\n",
        "    else:\n",
        "        return url\n",
        "def clean_and_format_facebook(url):\n",
        "    if url is not None:\n",
        "        # Remove any leading/trailing spaces\n",
        "        url = url.strip()\n",
        "\n",
        "        # Use regular expression to extract the portion starting with 'https://'\n",
        "        match = re.search(r'https://.*', url)\n",
        "        if match:\n",
        "            url = match.group(0)\n",
        "        else:\n",
        "            # If 'https://' is not found, consider the entire URL\n",
        "            url = None  # or url = '' if you want to keep it as an empty string\n",
        "\n",
        "    return url\n",
        "\n",
        "def convert_operation(text):\n",
        "  if text==None :\n",
        "    return None\n",
        "\n",
        "  def extract_days(text):\n",
        "    # Replace \"Friday\" with \"Fri\"\n",
        "    # Find all matches of day-day format\n",
        "      text = re.sub(r'\\bMonday\\b', 'Mon', text, flags=re.IGNORECASE)\n",
        "      text = re.sub(r'\\bTuesday\\b', 'Tue', text, flags=re.IGNORECASE)\n",
        "      text = re.sub(r'\\bWednesday\\b', 'Wed', text, flags=re.IGNORECASE)\n",
        "      text = re.sub(r'\\bThursday\\b', 'Thu', text, flags=re.IGNORECASE)\n",
        "      text = re.sub(r'\\bFriday\\b', 'Fri', text, flags=re.IGNORECASE)\n",
        "      text = re.sub(r'\\bSaturday\\b', 'Sat', text, flags=re.IGNORECASE)\n",
        "      text = re.sub(r'\\bSunday\\b', 'Sun', text, flags=re.IGNORECASE)\n",
        "\n",
        "\n",
        "      pattern = r'\\b(?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)\\s*(?:-|–|to)\\s*(?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)\\b|\\b(?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)\\b'\n",
        "      matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
        "\n",
        "      return matches\n",
        "  def split_list(input_list):\n",
        "\n",
        "      re=[]\n",
        "      for list in range(0,len(input_list),2):\n",
        "        if list + 1 < len(input_list):\n",
        "          first=input_list[list].replace(\" \",\"\")\n",
        "          second=input_list[list+1].replace(\" \",\"\")\n",
        "          final=first+\" to \"+second\n",
        "          re.append(final)\n",
        "    # Check for remaining elements\n",
        "  def extract_time_patterns(operating_hours):\n",
        "    # Define a regular expression to match time patterns\n",
        "      time_pattern = r'\\d+:\\d+\\s*[apm]+'\n",
        "\n",
        "    # Use regular expression to extract time patterns\n",
        "      time_patterns = re.findall(time_pattern, operating_hours)\n",
        "      time=split_list(time_patterns)\n",
        "\n",
        "\n",
        "      return time\n",
        "  days=extract_days(text)\n",
        "  time=extract_time_patterns(text)\n",
        "  def mapping(days,time):\n",
        "      days_mapping = {\n",
        "      'Mon': 0,\n",
        "      'Tue': 1,\n",
        "      'Wed': 2,\n",
        "      'Thu': 3,\n",
        "      'Fri': 4,\n",
        "      'Sat': 5,\n",
        "      'Sun': 6\n",
        "      }\n",
        "      result = [[] for _ in range(7)]\n",
        "      if len(days)==0:\n",
        "          return None\n",
        "      len_t=len(time)\n",
        "\n",
        "      for i in range(0,len(days)):\n",
        "          if i>=len_t:\n",
        "              return None\n",
        "\n",
        "          days[i] = ''.join([c if unicodedata.category(c) != 'Pd' else '-' for c in days[i]])\n",
        "          days[i]=days[i].replace(\"to\",\"-\")\n",
        "\n",
        "          if '-'  in days[i]:\n",
        "              start,end= days[i].split(\"-\")\n",
        "              start=start.replace(\" \",\"\")\n",
        "              end=end.replace(\" \",\"\")\n",
        "              start_i=days_mapping[start]\n",
        "              end_i=days_mapping[end]\n",
        "              for j in range(start_i,end_i+1):\n",
        "                  result[j].append(time[i])\n",
        "          else:\n",
        "              start=days[i].replace(\" \",\"\")\n",
        "              start_i=days_mapping[start]\n",
        "              result[start_i].append(time[i])\n",
        "\n",
        "      return result\n",
        "  def format_time(time_str):\n",
        "    if time_str:\n",
        "        # Capitalize 'am' and 'pm', add space before AM/PM\n",
        "        formatted_time = time_str.replace('am', 'AM').replace('pm', 'PM').replace('AM', ' AM').replace('PM', ' PM')\n",
        "        return formatted_time\n",
        "    return None\n",
        "  if days ==None or time==None:\n",
        "    return None\n",
        "  if len(days)!= len(time) :\n",
        "      return None\n",
        "\n",
        "  final_result=mapping(days,time)\n",
        "  if final_result ==None:\n",
        "    return None\n",
        "  result=[\" , \".join(map(str,sublist)) if sublist else None for sublist in final_result]\n",
        "  finall=[]\n",
        "  for i in result:\n",
        "    v=format_time(i)\n",
        "    finall.append(v)\n",
        "  return finall\n",
        "\n",
        "\n",
        "def union_and_process_lists(list1, list2):\n",
        "    if isinstance(list1, list) and isinstance(list2, list):\n",
        "        # Combine the lists without modifying the original values\n",
        "        list1=[item.strip().lower().replace(\"'\",\"\") for sublist in list1 for item in sublist.split(',')]\n",
        "        list2=[item.strip().lower().replace(\"'\",\"\") for sublist in list2 for item in sublist.split(',')]\n",
        "\n",
        "        union_lower = list(set(list1 + list2))\n",
        "        result_list = [value.strip().title() for value in union_lower]\n",
        "        return result_list\n",
        "    elif isinstance(list1, list):\n",
        "\n",
        "        return list1  # list1 is a list, return list1\n",
        "    elif isinstance(list2, list):\n",
        "        return list2  # list2 is a list, return list2\n",
        "\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def find_columns_with_empty_values(df):\n",
        "    empty_columns = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        if df[column].apply(lambda x: isinstance(x, str) and x == \"\").any():\n",
        "            empty_columns.append((column,\" have space\"))\n",
        "        elif df[column].apply(lambda x: isinstance(x, list) and len(x) == 0).any():\n",
        "            empty_columns.append((column,\"empty list\"))\n",
        "        elif df[column].apply(lambda x: x is None).any():\n",
        "            empty_columns.append((column,\"have none\"))\n",
        "\n",
        "    return empty_columns\n"
      ],
      "metadata": {
        "id": "rIHupx3WFC2L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "2ff09fae-1323-439a-8c9b-817dac8cbdc3",
          "showTitle": false,
          "title": ""
        },
        "id": "zkLEgTISmiEi"
      },
      "source": [
        "### 1.2 Exploratory Data Analysis 📊 [3]\n",
        "\n",
        "Perform a simple exploratory data analysis of each dataset to understand its structure, null values, nature of the data, data types, duplicate analysis, data quality and more."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### seeking information about table_a\n",
        " \\ accesing data type in our dataframe\n",
        "  checking for if duplicate of brand_name and latitude and longitude \\ present\\\n",
        " checking for area,address,city,categories,description for pattern\\\n",
        " checking how many have delivery partners\\\n",
        " checking for phone_number\\\n",
        " checking for facebook\\\n"
      ],
      "metadata": {
        "id": "NQRQ3gT_i_1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_a.info()\n",
        "print(find_columns_with_empty_values(table_a))\n",
        "result_df = table_a[table_a.duplicated(subset=['brand_name', 'latitude','longitude'], keep=False)]\n",
        "display(result_df.shape)\n",
        "display(table_a[[\"area\",\"address\",\"city\"]].head(3))\n",
        "display(table_a[\"categories\"].head(3))\n",
        "display(table_a[\"description\"].head(3))\n",
        "display(table_a[~table_a[\"has_delivery_partners\"]].head(2))\n",
        "display(table_a[\"phone_number\"].head(3))\n",
        "display(table_a[\"order_online_link\"].head(3))\n",
        "display(table_a[\"facebook\"].head(3))"
      ],
      "metadata": {
        "id": "WfjD7ZS7jF3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## checking for table_b\n",
        "checking for column if they contain None or space or list containing any None value\\\n",
        "checking for value area,address,city,categories,meals_offered,description,rating\\\n",
        "checking for different timezone and seeing city address for them\n",
        "checking for valid email_address and getting how many are not"
      ],
      "metadata": {
        "id": "3rdKqClTnUgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_b.info()\n",
        "print(find_columns_with_empty_values(table_b))\n",
        "result_df = table_a[table_b.duplicated(subset=['brand_name', 'latitude','longitude'], keep=False)]\n",
        "display(result_df.shape)\n",
        "\n",
        "display(table_b[[\"area\",\"address\",\"city\"]].head(3))\n",
        "display(table_b[\"categories\"].head(3))\n",
        "display(table_b[\"meals_offered\"].head(3))\n",
        "display(table_b[\"description\"].head(3))\n",
        "display(table_b[\"rating\"].value_counts())\n",
        "\n",
        "table_b_different_timezone=table_b[~(table_b[\"operating_timezone\"]==\"America/Los_Angeles\")]\n",
        "table_b_different_timezone=table_b_different_timezone[table_b_different_timezone[\"operating_timezone\"].notna()]\n",
        "display(table_b_different_timezone[[\"operating_timezone\",\"city\",\"address\"]])\n",
        "\n",
        "table_b['is_valid_email'] = table_b['email_address'].str.contains(r'^(?=.*@)(?=.*\\.)', na=True)\n",
        "value=table_b[\"email_address\"].value_counts()\n",
        "sum=value.sum()\n",
        "print(sum)\n",
        "print(table_b[\"is_valid_email\"].value_counts())\n",
        "invalid_email_rows = table_b['is_valid_email'] == False\n",
        "display(invalid_email_rows.shape)"
      ],
      "metadata": {
        "id": "BpMyCiW7nYsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## table_c checking\n",
        "getting basic info and data type of table\n",
        "checking for duplicated brandname,latitude and longitude\n",
        "getting info on area,address,city,categories,operating_hours\n",
        "checking for phone_number\n",
        "looking for review_count and review_count_not_recommended\n",
        "looking for pattern in is_claimed and is_claimable\n",
        "looking for review_count_by_rating"
      ],
      "metadata": {
        "id": "fZrOD1U5oedH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "table_c.info()\n",
        "print(find_columns_with_empty_values(table_c))\n",
        "result_df = table_a[table_c.duplicated(subset=['brand_name', 'latitude','longitude'], keep=False)]\n",
        "display(result_df.shape)\n",
        "display(table_c[[\"area\",\"address\",\"city\"]].head(3))\n",
        "display(table_c[\"categories\"].head(3))\n",
        "display(table_c[\"operating_hours_mon\"].value_counts())\n",
        "phone_pattern = r'^\\(\\d{3}\\) \\d{3}-\\d{4}$'\n",
        "invalid_numbers = table_c[\"telephone_number\"].str.match(phone_pattern, na=None)\n",
        "print(invalid_numbers.value_counts())\n",
        "display(table_c[[\"review_count\",\"review_count_not_recommended\"]])\n",
        "display(table_c[[\"is_claimed\",\"is_claimable\"]].value_counts())\n",
        "display(table_c[\"review_count_by_rating\"].head(3))"
      ],
      "metadata": {
        "id": "wez-KQqQoiEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## checking table_d\n",
        "getting data type of table\\\n",
        "finding column with empty spaces,none value,and list with none\\\n",
        "looking for area,address and city\\\n",
        "categories\n"
      ],
      "metadata": {
        "id": "j5C2ZwXDpUnQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c666aa55-88c5-4535-bc59-db736546ba66",
          "showTitle": false,
          "title": ""
        },
        "id": "iI_Z2FnYmiEi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "table_d.info()\n",
        "\n",
        "print(find_columns_with_empty_values(table_d))\n",
        "\n",
        "result_df = table_a[table_d.duplicated(subset=['brand_name', 'latitude','longitude'], keep=False)]\n",
        "display(result_df.shape)\n",
        "\n",
        "display(table_d[[\"area\",\"address\",\"city\"]].head(3))\n",
        "display(table_d[\"categories\"].head(3))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "table_a= table_a.applymap(lambda x: None if isinstance(x, str) and (x == '' or x.isspace()) else x)\n",
        "table_b=table_b.applymap(lambda x: None if isinstance(x, str) and (x == '' or x.isspace()) else x)\n",
        "table_c = table_c.applymap(lambda x: None if isinstance(x, str) and (x == '' or x.isspace()) else x)\n",
        "table_d = table_d.applymap(lambda x: None if isinstance(x, str) and (x == '' or x.isspace()) else x)"
      ],
      "metadata": {
        "id": "4FQqQjv4EmuU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cleaning up table_a a.k.a \"oscar\"\n",
        "removing buplicate if both address and brand_name are same\n",
        "removing duplicates if brand_name and latitude and longitude are same\n",
        "filtering our area column getting data from address first phrase before \",\"\\\n",
        "handling our address removing abnormality for them as we have noticed like removing \", ,\" and putting CA in right place as formatted\n",
        "correcting our has_delivery_partners ,if we have length of deliver_partner then True\n",
        "correcting our phone_number in correct format of (ddd) ddd-dddd\n",
        "removing html tags from entertainment column and also removing N/A if present\n",
        "handling our order_online_link adding www where required\n",
        "formating our facebook-link\n",
        "handling our menu_url with check_url_and_none function\n",
        "* parsing our raw data in operating_column and getting data according to it and putting in 7 different column (as mon,tue........)"
      ],
      "metadata": {
        "id": "bypVOcIXEWi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_a= table_a.drop_duplicates(subset=['brand_name', 'address'], keep='first')\n",
        "\n",
        "table_a = table_a.drop_duplicates(subset=['brand_name', 'latitude','longitude'], keep='first')\n",
        "\n",
        "handling_html_tags(table_a,\"description\")\n",
        "\n",
        "table_a[\"categories\"]=table_a[\"categories\"].apply(keep_unique_values)\n",
        "table_a['area_1'] = table_a['address'].str.split(',').str[0]\n",
        "table_a = table_a.drop('area', axis=1)\n",
        "# Rename 'area_1' to 'area'\n",
        "table_a = table_a.rename(columns={'area_1': 'area'})\n",
        "table_a[\"address\"]=table_a[\"address\"].apply(handling_address)\n",
        "\n",
        "table_a[\"has_delivery_partners\"]=table_a[\"delivery_partners\"].apply(lambda value:True if (value is not None and len(value)>0) else False)\n",
        "\n",
        "check_number(table_a,\"phone_number\")\n",
        "table_a[\"phone_number\"]=table_a[\"phone_number\"].apply(format_phone_number)\n",
        "phone_pattern = r'^\\(\\d{3}\\) \\d{3}-\\d{4}$'\n",
        "table_a[\"phone_number\"] = table_a[\"phone_number\"].str.strip()  # Strip leading and trailing spaces\n",
        "table_a[\"phone_number\"]=table_a[\"phone_number\"].apply(format_phone_number)\n",
        "invalid_numbers = table_a[\"phone_number\"].str.match(phone_pattern, na=None).all()\n",
        "print(invalid_numbers)\n",
        "\n",
        "table_a[\"entertainment\"] = table_a[\"entertainment\"].apply(remove_html_tags)\n",
        "table_a[\"entertainment\"]=table_a[\"entertainment\"].apply(lambda value : None if value == ''or value ==\"N/A\" else value)\n",
        "\n",
        "table_a[\"order_online_link\"]=table_a[\"order_online_link\"].apply(add_www)\n",
        "\n",
        "table_a[\"facebook\"]=table_a[\"facebook\"].apply(clean_and_format_facebook)\n",
        "\n",
        "check_url(table_a,\"menu_url\")\n",
        "check_url_change_none(table_a,\"menu_url\")\n",
        "\n",
        "column_names = ['mon', 'tue', 'wed', 'thu','fri','sat','sun']\n",
        "\n",
        "# Create an empty DataFrame with the specified column names\n",
        "df = pd.DataFrame(columns=column_names)\n",
        "\n",
        "handling_space(table_a,\"operating_hours\")\n",
        "\n",
        "table_a[\"operating_hours\"]=table_a[\"operating_hours\"].apply(convert_operation)\n",
        "\n",
        "\n",
        "for day, col_name in zip([0, 1, 2, 3, 4, 5, 6],\n",
        "                        [f'operating_hours_{day.lower()}' for day in ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']]):\n",
        "    table_a[col_name] = table_a['operating_hours'].apply(lambda x: (x[day] if x and day<len(x) else None))\n"
      ],
      "metadata": {
        "id": "W4xkdnMJE0Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cleaning up table_b a.k.a \"tango\"\n",
        "dropping duplicate if both brand_name and address are same\\\n",
        "droping duplicates if our brand_name and latitude and longitude are same\\\n",
        "handling our area ,getting from address column, first phrase before \",\"\n",
        "\n",
        "sorting our meals_offered column so list having same data in different index are treated same\n",
        "handling our invalid_email getting how many are different and putting none if not\n",
        "formatting our phone_number\n",
        "making our rating column free from negative number and having absolute value (i have not put them as 0 if negative)\n",
        "looking for different_time_zone and looking if they match with address and city"
      ],
      "metadata": {
        "id": "YIPlTygAF5_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_b= table_b.drop_duplicates(subset=['brand_name', 'address'], keep='first')\n",
        "\n",
        "table_b = table_b.drop_duplicates(subset=['brand_name', 'latitude','longitude'], keep='first')\n",
        "\n",
        "table_b['area_1'] = table_b['address'].str.split(',').str[0]\n",
        "table_b = table_b.drop('area', axis=1)\n",
        "table_b = table_b.rename(columns={'area_1': 'area'})\n",
        "table_b = table_b.where(pd.notna(table_b), None)\n",
        "\n",
        "table_b[\"meals_offered\"]=table_b[\"meals_offered\"].apply(lambda lst: sorted(lst) if lst else None)\n",
        "\n",
        "table_b['is_valid_email'] = table_b['email_address'].str.contains(r'^(?=.*@)(?=.*\\.)', na=True)\n",
        "value=table_b[\"email_address\"].value_counts()\n",
        "sum=value.sum()\n",
        "print(sum)\n",
        "print(table_b[\"is_valid_email\"].value_counts())\n",
        "invalid_email_rows = table_b['is_valid_email'] == False\n",
        "display(invalid_email_rows.shape)\n",
        "table_b.loc[invalid_email_rows, 'email_address'] = None\n",
        "\n",
        "table_b[\"telephone_number\"]=table_b[\"telephone_number\"].apply(format_phone_number)\n",
        "\n",
        "handling_html_tags(table_b,\"description\")\n",
        "\n",
        "table_b[\"rating\"]=table_b[\"rating\"].apply(lambda value: abs(value))\n",
        "\n",
        "table_b_different_timezone=table_b[~(table_b[\"operating_timezone\"]==\"America/Los_Angeles\")]\n",
        "table_b_different_timezone=table_b_different_timezone[table_b_different_timezone[\"operating_timezone\"].notna()]"
      ],
      "metadata": {
        "id": "Ucg5aM2lGHvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cleaning up table_c a.k.a \"yankee\"\n",
        "\n",
        "dropping duplicate if both brand_name and address are same\\\n",
        "droping duplicates if our brand_name and latitude and longitude are same\\\n",
        "putting data in our area column from address, first phrase before \",\"\n",
        "checking our operating_hours_xxx for closed first finding in which columns we have to perform and then removing Closed from them\n",
        "handling address , adding city in them\n",
        "formatting our phone number getting how many are not compatible with our pattern\n",
        "handling our address column replacing city at its right place\n",
        "handling review_count ,subtracting review_count from review_not_recommended handling negative number with zero\n",
        "making our is_claimed right by comparing with is_claimable and putting value if is_claimed is false and and is_claimable is true\n",
        "handling year established if we have year below 1785 removing them\n",
        "handling review_count_by_rating into five column normalizing as [\"terrible,poor,average,very_good,excellent]\n"
      ],
      "metadata": {
        "id": "zlcLMIzVGkB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_c= table_c.drop_duplicates(subset=['brand_name', 'address'], keep='first')\n",
        "\n",
        "table_c = table_c.drop_duplicates(subset=['brand_name', 'latitude','longitude'], keep='first')\n",
        "\n",
        "# table_c = table_c.where(pd.notna(table_c), None)\n",
        "\n",
        "table_c['area_1'] = table_c['address'].str.split(',').str[0]\n",
        "table_c = table_c.drop('area', axis=1)\n",
        "table_c = table_c.rename(columns={'area_1': 'area'})\n",
        "\n",
        "\n",
        "columns_to_check = ['operating_hours_mon', 'operating_hours_tue', 'operating_hours_wed', 'operating_hours_thu', 'operating_hours_fri', 'operating_hours_sat', 'operating_hours_sun']\n",
        "\n",
        "# Check and replace \"Closed\" with None in the specified columns\n",
        "table_c[columns_to_check] = table_c[columns_to_check].applymap(lambda x: None if isinstance(x, str) and \"Closed\" in x else x)\n",
        "table_c = table_c.applymap(lambda x: None if isinstance(x, str) and \"Closed\" in x else x)\n",
        "\n",
        "\n",
        "table_c[\"address\"]=table_c.apply(replace_city,axis=1)\n",
        "\n",
        "check_number(table_c,\"telephone_number\")\n",
        "table_c[\"telephone_number\"]=table_c[\"telephone_number\"].apply(format_phone_number)\n",
        "phone_pattern = r'^\\(\\d{3}\\) \\d{3}-\\d{4}$'\n",
        "table_c[\"telephone_number\"] = table_c[\"telephone_number\"].str.strip()  # Strip leading and trailing spaces\n",
        "table_c[\"telephone_number\"]=table_c[\"telephone_number\"].apply(format_phone_number)\n",
        "invalid_numbers = table_c[\"telephone_number\"].str.match(phone_pattern, na=None)\n",
        "print(invalid_numbers.value_counts())\n",
        "\n",
        "table_c['review_count'] = table_c['review_count'] - table_c['review_count_not_recommended']\n",
        "table_c['review_count'] = table_c['review_count'].apply(lambda x: max(x, 0))\n",
        "\n",
        "table_c['is_claimed'] = table_c.apply(lambda row: False if row['is_claimable'] == True and row['is_claimed'] == False else row['is_claimed'], axis=1)\n",
        "table_c=table_c.drop('is_claimable', axis=1, inplace=True)\n",
        "# table_c[\"year_established\"]=table_c[\"year_established\"].apply(lambda value : None if (value == None or int(value)<1785) else str(value)[0:4])\n",
        "\n",
        "# table_c[\"review_check\"]=table_c[\"review_count_by_rating\"].apply(lambda lst: True if  lst is not None and len(lst)==5 else False)\n",
        "\n",
        "\n",
        "table_c = table_c.join(table_c['review_count_by_rating'].apply(pd.Series).rename(columns=lambda x: f'new_column_{x}'))\n",
        "column_mapping = {\n",
        "    'new_column_0': 'terrible_review_count',\n",
        "    'new_column_1': 'poor_review_count',\n",
        "    'new_column_2': 'average_review_count',\n",
        "    'new_column_3': 'very_good_review_count',\n",
        "    'new_column_4': 'excellent_review_count'\n",
        "}\n",
        "\n",
        "\n",
        "table_c.rename(columns=column_mapping, inplace=True)"
      ],
      "metadata": {
        "id": "4tp8lgUIGxxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cleaning up table_d a.k.a \"delta\"\n",
        "converting na as none\n",
        "removing duplicates if brand_name and latitude and longitude are same\n",
        "removing duplicates if brand_name and address are same\n",
        "cleaning our categories column removing unwanted character from them"
      ],
      "metadata": {
        "id": "V31865g8IFYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_d['area'] = table_d['area'].astype(object)\n",
        "table_d = table_d.where(pd.notna(table_d), None)\n",
        "\n",
        "def clean_category_list(category_str):\n",
        "    # Remove empty strings, extra spaces, and single quotes from each element\n",
        "    if category_str == None:\n",
        "      return None\n",
        "    category_list = ast.literal_eval(category_str)\n",
        "    cleaned_list = [category.strip().replace(\"'\", \"\") for category in category_list if category.strip()]\n",
        "    return cleaned_list\n",
        "\n",
        "# Apply the clean_category_list function to the 'categories' column\n",
        "table_d= table_d.drop_duplicates(subset=['brand_name', 'address'], keep='first')\n",
        "table_d= table_d.drop_duplicates(subset=['brand_name', 'latitude','longitude'], keep='first')\n",
        "table_d['categories'] = table_d['categories'].apply(clean_category_list)"
      ],
      "metadata": {
        "id": "fj99LyshIbNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IAhskVBNEdsA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "1bb3df62-e5e1-42ba-92b2-ae527c1d19fe",
          "showTitle": false,
          "title": ""
        },
        "id": "D3Y_DdFtmiEi"
      },
      "source": [
        "## Section 2: Join and Knit 🧵 [15]\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/rytLWOErAX1F6/giphy.gif\" width=\"350px\" alt=\"merge\">\n",
        "<br></br>\n",
        "\n",
        "1. **Objective**:\n",
        "   - Merge the datasets - Delta, Oscar, Tango, and Yankee into one final restaurant dataset.\n",
        "   - Use the *Lookup Records* table for reference on joining the source tables.\n",
        "\n",
        "2. **Key Details**:\n",
        "   - Datasets contain unique details (e.g., Dining Style, Meals Offered).\n",
        "   - Some details (e.g., Name, Location) are shared across datasets.\n",
        "   - Your goal: Merge to create `Table Z`.\n",
        "\n",
        "3. **How to Merge**:\n",
        "   - Join tables Delta through Yankee using **all columns**.\n",
        "   - Address overlapping columns to avoid duplication.\n",
        "   - Merge columns with similar data but different names (like 'telephone_number' and 'phone_number').\n",
        "   - If a column appears in multiple tables with varying values, select the most appropriate one (e.g., 'price_range' and 'price_class').\n",
        "\n",
        "### Example\n",
        "\n",
        "Consider the hypothetical tables A, B, C, and D. We aim to merge these into Table Z.\n",
        "\n",
        "Note: The example highlights specific columns from A to D, but you must consider all columns from datasets Delta to Yankee.\n",
        "\n",
        "**Table A**\n",
        "\n",
        "| id | restaurant_name | area | categories | rating | website | price_range | executive_chef |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| 1 | The Good Food | Downtown | Italian,Seafood | 4.5 | goodfood.com | $11-30 | John Doe |\n",
        "| 2 | Taste of Asia | Midtown | Asian,Thai | 4.0 | tasteofasia.com | $10-25 | Jane Doe |\n",
        "\n",
        "**Table B**\n",
        "\n",
        "| id | name | location | type | rating | website | price_class | meals_offered |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| 1 | The Good Food | Downtown | Italian | 4.5 | goodfood.com | 2 | Lunch,Dinner |\n",
        "| 3 | Spicy Hut | Uptown | Indian,Spicy | 4.2 | spicyhut.com | 3 | Lunch,Dinner,Brunch |\n",
        "\n",
        "**Table C**\n",
        "\n",
        "| id | restaurant | location | categories | telephone_number | price_range |\n",
        "| --- | --- | --- | --- | --- | --- |\n",
        "| 2 | Taste of Asia | Midtown | Asian,Thai | 1234567890 | $10-25 |\n",
        "| 4 | Fresh & Healthy | Suburbs | Vegetarian,Vegan | 9876543210 | $15-30 |\n",
        "\n",
        "**Table D**\n",
        "\n",
        "| id | restaurant_name | area | type | phone_number | website | executive_chef |\n",
        "| --- | --- | --- | --- | --- | --- | --- |\n",
        "| 3 | Spicy Hut | Uptown | Indian | 1234567890 | spicyhut.com | Raj Patel |\n",
        "| 4 | Fresh & Healthy | Suburbs | Vegetarian | 9876543210 | freshandhealthy.com | Emily Brown |\n",
        "\n",
        "We want to merge tables A, B, C, and D into a single table Z. However, we need to handle the overlapping and exclusive columns carefully to avoid duplication and to ensure a single source of truth. We will use coalescing to handle overlapping columns with different names (e.g., 'telephone_number' and 'phone_number') and combine values when necessary (e.g., 'categories' and 'type'). We will also choose the most appropriate value for columns that appear in multiple tables but have different values (e.g., 'price_range' and 'price_class').\n",
        "\n",
        "**Table Z**\n",
        "\n",
        "| id | name | location | categories | rating | website | price_range | executive_chef | meals_offered | telephone_number |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| 1 | The Good Food | Downtown | Italian,Seafood | 4.5 | goodfood.com | $11-30 | John Doe | Lunch,Dinner | |\n",
        "| 2 | Taste of Asia | Midtown | Asian,Thai | 4.0 | tasteofasia.com | $10-25 | Jane Doe | | 1234567890 |\n",
        "| 3 | Spicy Hut | Uptown | Indian,Spicy | 4.2 | spicyhut.com | $25-40 | Raj Patel | Lunch,Dinner,Brunch | 1234567890 |\n",
        "| 4 | Fresh & Healthy | Suburbs | Vegetarian,Vegan | | freshandhealthy.com | $15-30 | Emily Brown | | 9876543210 |\n",
        "\n",
        "\n",
        "> 📝 It's important to note that while this example is straightforward, you may encounter some ambiguity when working with the actual datasets Delta through Yankee. **The main objective is to construct a dataset that provides a comprehensive and detailed context for each restaurant**. There will be instances where you'll need to make subjective decisions, such as prioritizing one dataset over another or creating a new field to harmonize different representations of the same data across datasets (e.g., your own classification of price range). These decisions are up to you, and we encourage you to develop your own approach and rationale for resolving these issues. Just make sure to explain your choices and reasoning in a markdown cell.\n",
        "\n",
        "---\n",
        "\n",
        "### Tips 💡\n",
        "- Remember to clean up columns in the 4 source datasets where necessary that would make the join more seamless, instead of retrofitting ad-hoc fixes after the fact.\n",
        "- If you have two or more columns with same or similar names or two or more different columns that are representing the same kind of data - then you have not reached the final output\n",
        "- All values in a single column should have the same data type. A type can be simple like `integer`, `float`, `string`, or complex like `list[str]` or `list[int]`. Try to avoid columns that result in having `json` or `dict` values.\n",
        "- The underlying four datasets and lookup records may have duplicates, so watch out for bugs in the join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "7fe0ed02-a238-4884-9ac4-69cac62805c3",
          "showTitle": false,
          "title": ""
        },
        "id": "rfyBrJnNmiEj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### first merging on yankee and tango"
      ],
      "metadata": {
        "id": "4yW7q7I-LF6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#left merge on table_c on table_c and then on table_b\n",
        "merged_df = table_e.merge(table_c, left_on='yankee', right_on='id', how='left'). \\\n",
        "    merge(table_b, left_on='tango', right_on='id', how='left',suffixes=(\"_yan\",\"_tan\"))"
      ],
      "metadata": {
        "id": "EKi1imeTLLgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking for same latitude and longitude and filtering out if\n",
        "filtered_df = merged_df[(merged_df['latitude_yan'] == merged_df['latitude_tan']) & (merged_df['longitude_yan'] == merged_df['longitude_tan'])]\n",
        "display(filtered_df.head(3))\n",
        "display(filtered_df.shape)\n",
        "filtered_df = merged_df[(merged_df['id_yan'].notna()) & (merged_df['id_tan'].notna())]\n",
        "filtered_df = filtered_df[filtered_df['id_yan'].isin(filtered_df['id_tan'])]\n",
        "display(filtered_df.shape)"
      ],
      "metadata": {
        "id": "S-Qp6SWBLPll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dictionat for having same type of data like area\\,latitude .......\n",
        "#creating two dictionary one for categorical value second for int float type\n",
        "# for categorical if none is present in our parent table we will fill up by second\n",
        "# for numerical we will put maximum of them\n",
        "# for list type we will union them keeping unique of them\n",
        "# removing unnenecasry columns\n",
        "columns_to_drop=[]\n",
        "columns_to_fill = {\n",
        "    'subregion_yan': 'subregion_tan',\n",
        "    'area_yan': 'area_tan',\n",
        "    'city_yan':'city_tan',\n",
        "    'brand_name_yan':'brand_name_tan',\n",
        "    'latitude_yan':'latitude_tan',\n",
        "    'longitude_yan':'longitude_tan',\n",
        "    'address_yan':'address_tan',\n",
        "    'website_url':'restaurant_website',\n",
        "    'is_claimed':'claimed',\n",
        "    'telephone_number_yan':'telephone_number_tan',\n",
        "    'description_yan':'description_tan',\n",
        "    'operating_hours_mon_yan':'operating_hours_mon_tan',\n",
        "    'operating_hours_tue_yan':'operating_hours_tue_tan',\n",
        "    'operating_hours_wed_yan':'operating_hours_wed_tan',\n",
        "    'operating_hours_thu_yan':'operating_hours_thu_tan',\n",
        "    'operating_hours_fri_yan':'operating_hours_fri_tan',\n",
        "    'operating_hours_sat_yan':'operating_hours_sat_tan',\n",
        "    'operating_hours_sun_yan':'operating_hours_sun_tan'\n",
        "\n",
        "}\n",
        "columns_to_drop = ['place_features_tan','address_tan','rating_tan','very_good_review_count_tan','review_count_tan','excellent_review_count_tan','average_review_count_tan','poor_review_count_tan','terrible_review_count_tan','operating_hours_sun_tan','operating_hours_sat_tan','operating_hours_fri_tan','operating_hours_thu_tan','operating_hours_wed_tan','operating_hours_tue_tan','operating_hours_mon_tan','description_tan','telephone_number_tan','claimed','subregion_tan', 'area_tan','city_tan','brand_name_tan','categories_tan','latitude_tan','longitude_tan','restaurant_website']\n",
        "int_values={  'terrible_review_count_yan':'terrible_review_count_tan',\n",
        "    'poor_review_count_yan':'poor_review_count_tan',\n",
        "    'average_review_count_yan':'average_review_count_tan',\n",
        "    'very_good_review_count_yan':'very_good_review_count_tan',\n",
        "    'excellent_review_count_yan':'excellent_review_count_tan',\n",
        "    'review_count_yan':'review_count_tan',\n",
        "    'rating_yan':'rating_tan',\n",
        "     'very_good_review_count_yan':'very_good_review_count_tan'         }\n",
        "# Iterate over the columns to fill\n",
        "for col_to_fill, col_source in columns_to_fill.items():\n",
        "    columns_to_drop.append(col_source)\n",
        "    merged_df[col_to_fill].fillna(merged_df[col_source], inplace=True)\n",
        "\n",
        "for col_to_fill,col_source in int_values.items():\n",
        "  columns_to_drop.append(col_source)\n",
        "  merged_df[col_to_fill] = np.where(\n",
        "    (merged_df[col_to_fill].isna() & merged_df[col_source].isna()),  # Check if both are None\n",
        "    None,  # Set col_to_fill to None if both are None\n",
        "    np.where(\n",
        "        (merged_df[col_to_fill].isna()) | (merged_df[col_to_fill] < merged_df[col_source]),\n",
        "        merged_df[col_source],\n",
        "        merged_df[col_to_fill]\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "merged_df['categories_yan'] = merged_df.apply(lambda row: union_and_process_lists(row['categories_yan'], row['categories_tan']), axis=1)\n",
        "merged_df['palce_feature_yan'] = merged_df.apply(lambda row: union_and_process_lists(row['place_features_yan'], row['place_features_tan']), axis=1)\n",
        "columns_to_drop.extend([\"categories_tan\",\"place_features_tan\"])\n",
        "merged_df.drop(columns=columns_to_drop, inplace=True)"
      ],
      "metadata": {
        "id": "SnxczlAaLXCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### second merging on delta"
      ],
      "metadata": {
        "id": "VhhxeTSHMAgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#left merge on table_e on table_c and then on table_d\n",
        "merged_df = merged_df.merge(table_d, left_on='delta', right_on='id', how='left',suffixes=(\"\",\"_del\"))"
      ],
      "metadata": {
        "id": "soXyYJpmMJob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking for same latitude and longitude and filtering out if\n",
        "filtered_df = merged_df[(merged_df['id_tan'].notna()) & (merged_df['id'].notna())]\n",
        "filtered_df = filtered_df[filtered_df['id_tan'].isin(filtered_df['id'])]\n",
        "display(filtered_df.shape)\n",
        "filtered_df = merged_df[(merged_df['id_yan'].notna()) & (merged_df['id'].notna())]\n",
        "filtered_df = filtered_df[filtered_df['id_yan'].isin(filtered_df['id'])]\n",
        "display(filtered_df.shape)"
      ],
      "metadata": {
        "id": "KiyZc-TmMRZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dictionat for having same type of data like area\\,latitude .......\n",
        "#creating two dictionary one for categorical value second for int float type\n",
        "# for categorical if none is present in our parent table we will fill up by second\n",
        "# for numerical we will put maximum of them\n",
        "# for list type we will union them keeping unique of them\n",
        "# removing unnenecasry columns\n",
        "column_to_drop=[]\n",
        "columns_to_fill = {\n",
        "    'industry':'industry_del',\n",
        "    'subregion_yan': 'subregion',\n",
        "    'area_yan': 'area',\n",
        "    'city_yan':'city',\n",
        "    'brand_name_yan':'brand_name',\n",
        "    'latitude_yan':'latitude',\n",
        "    'longitude_yan':'longitude',\n",
        "    'address_yan':'address'\n",
        "\n",
        "\n",
        "}\n",
        "for col_to_fill, col_source in columns_to_fill.items():\n",
        "    column_to_drop.append(col_source)\n",
        "    merged_df[col_to_fill].fillna(merged_df[col_source], inplace=True)\n",
        "merged_df['categories_yan'] = merged_df.apply(lambda row: union_and_process_lists(row['categories_yan'], row['categories']), axis=1)\n",
        "merged_df['palce_features_yan'] = merged_df.apply(lambda row: union_and_process_lists(row['place_features_yan'], row['palce_feature_yan']), axis=1)\n",
        "\n",
        "\n",
        "int_values={\n",
        "    'rating_yan':'rating'\n",
        "}\n",
        "for col_to_fill,col_source in int_values.items():\n",
        "  column_to_drop.append(col_source)\n",
        "  merged_df[col_to_fill] = np.where(\n",
        "    (merged_df[col_to_fill].isna() & merged_df[col_source].isna()),  # Check if both are None\n",
        "    None,  # Set col_to_fill to None if both are None\n",
        "    np.where(\n",
        "        (merged_df[col_to_fill].isna()) | (merged_df[col_to_fill] < merged_df[col_source]),\n",
        "        merged_df[col_source],\n",
        "        merged_df[col_to_fill]\n",
        "    )\n",
        ")\n",
        "\n",
        "column_to_drop.extend([\"categories\",\"palce_feature_yan\"])\n",
        "\n",
        "\n",
        "merged_df.drop(columns=column_to_drop, inplace=True)"
      ],
      "metadata": {
        "id": "GR-0ZqfDMb_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### last merging on oscar"
      ],
      "metadata": {
        "id": "01HTqaQZMjnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.merge(table_a, left_on='oscar', right_on='id', how='left',suffixes=(\"\",\"_osc\"))"
      ],
      "metadata": {
        "id": "hwvauuLrMq_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dictionat for having same type of data like area\\,latitude .......\n",
        "#creating two dictionary one for categorical value second for int float type\n",
        "# for categorical if none is present in our parent table we will fill up by second\n",
        "# for numerical we will put maximum of them\n",
        "# for list type we will union them keeping unique of them\n",
        "# removing unnenecasry columns\n",
        "column_to_drop=[]\n",
        "columns_to_fill = {\n",
        "    'subregion_yan': 'subregion',\n",
        "    'area_yan': 'area',\n",
        "    'city_yan':'city',\n",
        "    'brand_name_yan':'brand_name',\n",
        "    'latitude_yan':'latitude',\n",
        "    'longitude_yan':'longitude',\n",
        "    'address_yan':'address',\n",
        "    'description_yan':'description',\n",
        "    'website_url':'restaurant_website',\n",
        "    'telephone_number_yan':'phone_number',\n",
        "\n",
        "\n",
        "    'offers_delivery':'has_delivery_partners',\n",
        "    'offers_pickup':'has_pickup',\n",
        "    'restaurant_provided_menu_link':'menu_url',\n",
        "    'operating_hours_mon_yan':'operating_hours_mon',\n",
        "    'operating_hours_tue_yan':'operating_hours_tue',\n",
        "    'operating_hours_wed_yan':'operating_hours_wed',\n",
        "    'operating_hours_thu_yan':'operating_hours_thu',\n",
        "    'operating_hours_fri_yan':'operating_hours_fri',\n",
        "    'operating_hours_sat_yan':'operating_hours_sat',\n",
        "    'operating_hours_sun_yan':'operating_hours_sun',\n",
        "    'cross_streets':'cross_street'\n",
        "}\n",
        "\n",
        "int_values={\n",
        "    'rating_yan':'rating',\n",
        "    'rating_count':'rating_count_osc',\n",
        "    'atmosphere_rating':'atmosphere_rating_osc',\n",
        "    'food_rating':'food_rating_osc',\n",
        "    'service_rating':'service_rating_osc',\n",
        "    'value_rating':'value_rating_osc',\n",
        "    'terrible_review_count_yan':'terrible_review_count',\n",
        "    'poor_review_count_yan':'poor_review_count',\n",
        "    'average_review_count_yan':'average_review_count',\n",
        "    'very_good_review_count_yan':'very_good_review_count',\n",
        "    'excellent_review_count_yan':'excellent_review_count',\n",
        "\n",
        "}\n",
        "for col_to_fill, col_source in columns_to_fill.items():\n",
        "    merged_df[col_to_fill].fillna(merged_df[col_source], inplace=True)\n",
        "    column_to_drop.append(col_source)\n",
        "\n",
        "for col_to_fill,col_source in int_values.items():\n",
        "  column_to_drop.append(col_source)\n",
        "  merged_df[col_to_fill] = np.where(\n",
        "    (merged_df[col_to_fill].isna() & merged_df[col_source].isna()),  # Check if both are None\n",
        "    None,  # Set col_to_fill to None if both are None\n",
        "    np.where(\n",
        "        (merged_df[col_to_fill].isna()) | (merged_df[col_to_fill] < merged_df[col_source]),\n",
        "        merged_df[col_source],\n",
        "        merged_df[col_to_fill]\n",
        "    )\n",
        ")\n",
        "merged_df['categories_yan'] = merged_df.apply(lambda row: union_and_process_lists(row['categories_yan'], row['categories']), axis=1)\n",
        "\n",
        "column_to_drop.extend([\"categories\",\"review_count_not_recommended\"])"
      ],
      "metadata": {
        "id": "uinsfN5qMzAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.drop(columns=column_to_drop,inplace=True)"
      ],
      "metadata": {
        "id": "t4YHRNinM3H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we have mapped our price_range to common on analysis\n",
        "reversed_mapping = {\n",
        "    \"Inexpensive\": \"$11-30\",\n",
        "    \"$11-30\": \"$11-30\",\n",
        "    \"$31-60\": \"$31-60\",\n",
        "    \"Pricey\": \"$31-60\",\n",
        "    \"$$\": \"$11-30\",\n",
        "    \"Moderate\": \"$11-30\",\n",
        "    \"Ultra High-End\": \"Above $61\",\n",
        "    \"Above $61\": \"Above $61\",\n",
        "    \"Under $10\": \"$11-30\",\n",
        "    \"$30 and under\":\"$11-30\",\n",
        "    \"$31 to $50\":\"$31-60\",\n",
        "    \"$50 and over\":\"Above $61\",\n",
        "    \"$\":\"$11-30\",\n",
        "    \"$$ - $$$\":\"$31-60\",\n",
        "    \"$$$$\":\"Above $61\",\n",
        "    \"$$$\":\"$31-60\"\n",
        "}\n",
        "columns_to_map = ['price_range_yan', 'price_range_tan', 'price_range', 'price_range_osc']\n",
        "def map_values(row):\n",
        "    for column in columns_to_map:\n",
        "        if row[column] is not None:\n",
        "            return reversed_mapping.get(row[column])\n",
        "\n",
        "merged_df['mapped_range'] = merged_df.apply(map_values, axis=1)\n",
        "columns_to_map.append(\"mapped_range\")"
      ],
      "metadata": {
        "id": "fHqWBFiGM8eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handling operating_hours from table_b as it was not so conclusive\n",
        "selected_columns = ['operating_hours_mon_yan', 'operating_hours_sun_yan', 'operating_hours_tue_yan', 'operating_hours_wed_yan', 'operating_hours_thu_yan', 'operating_hours_fri_yan', 'operating_hours_sat_yan']\n",
        "\n",
        "# Check if all selected columns are None for each row\n",
        "mask = merged_df[selected_columns].isna().all(axis=1)\n",
        "\n",
        "# Fill the selected columns with the corresponding values from 'operating_hours' for rows where all selected columns are None\n",
        "merged_df.loc[mask, selected_columns] = merged_df.loc[mask, 'operating_hours']\n",
        "merged_df['description_yan'].fillna(merged_df['brand_description'], inplace=True)"
      ],
      "metadata": {
        "id": "MBvSpcuqND21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_to_drop=[\"operating_timezone\",\"is_network_non_bookable\",\"is_permanently_closed\",\"has_proof_of_vaccination_outdoor\",\"price_range_id\",\"is_convenience_store\",\"offers_package_returns\",\"offers_cannabis\",\"awards\",\"oscar\",\"tango\",\"delta\",\"yankee\",\"id_yan\",\"review_count_by_rating\",\"industry\",\"id_tan\",\"is_valid_email\",\"id\",\"operating_hours_osc\",\"id_osc\",'price_range_yan', 'price_range_tan', 'price_range', 'price_range_osc','brand_description','operating_hours']\n",
        "merged_df.reset_index()\n",
        "merged_df.drop(columns=column_to_drop,inplace=True)"
      ],
      "metadata": {
        "id": "LhJImw7aNI3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in merged_df.columns:\n",
        "    if col.endswith('yan'):\n",
        "        new_col_name = col.replace('_yan', '')\n",
        "        merged_df.rename(columns={col: new_col_name}, inplace=True)"
      ],
      "metadata": {
        "id": "Yrk9obgENNcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing any duplicate if brand_name and address are same\n",
        "# giving id\n",
        "result_df = merged_df[merged_df.duplicated(subset=['brand_name', 'address'], keep=False)]\n",
        "print(result_df.shape)\n",
        "merged_df=merged_df.drop_duplicates(subset=['brand_name', 'address'], keep='first')\n",
        "merged_df = merged_df.drop_duplicates(subset=['brand_name', 'latitude','longitude'], keep='first')\n",
        "merged_df=merged_df.rename(columns={\"facebook\":\"facebook_link\"})\n",
        "merged_df['id'] = range(1, len(merged_df) + 1)"
      ],
      "metadata": {
        "id": "QXbiCP4LNTMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c6311a13-fd83-4ec1-86ea-a241ce1a0419",
          "showTitle": false,
          "title": ""
        },
        "id": "nj8cjC1MmiEk"
      },
      "source": [
        "## Section 3: Web Harvesting 🕸 [15]\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/wSeaJHrOckToQ/giphy.gif\" width=\"350px\" alt=\"merge\">\n",
        "\n",
        "In this section, your task is to enhance the current dataset by navigating to the restaurant websites specified in the data, retrieving any incomplete information, and collecting additional features that could be essential for the analysis. This will involve accessing the URLs of restaurant websites listed in the dataset, and potentially exploring linked pages from the initial page to gather required information. The depth of pages you explore is at your discretion, as long as the added information substantially benefits the final dataset.\n",
        "\n",
        "Tasks include:\n",
        "\n",
        "- **Filling Missing Data**: Identify and fill in missing values in the dataset, such as restaurant descriptions, operating hours, and contact information.\n",
        "- **Extracting New Features**: Extract additional valuable information from the restaurant websites, such as:\n",
        "  - **Social Media Handles**: Links to Facebook, Twitter, Instagram, LinkedIn pages.\n",
        "  - **Awards and Recognitions**: Any awards or recognitions received by the restaurant.\n",
        "  - **Special Events or Offers**: Information on any special events, promotions, or discounts.\n",
        "  - Anything else that you feel may be relevant and valuable\n",
        "\n",
        "Be respectful of the website's `robots.txt` file and terms of service, and ensure your web scraping script is efficient, accurate, and does not overload the servers. You will be graded on the efficiency and accuracy of your script, as well as the completeness and usefulness of the extracted data. Ensure to update the dataset without altering its existing structure and content.\n",
        "\n",
        "> You may need to additionally join and clean up the resultant dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## function\n",
        "\n",
        "1. categorize_links()\\\n",
        "Function to categorize links on a webpage using string matching\n",
        "\n",
        "2. findPhone()\\\n",
        "Function to scrap phone number from webpages using regex for matching\n",
        "\n",
        "3. findEmail()\\\n",
        "Function to scrap email from webpages using regex for matching\n",
        "\n",
        "4. findBooleanValues()\\\n",
        "Function to scrap values which are stored as True/False from webpages using regex for matching\n",
        "\n",
        "5. findYear()\\\n",
        "Function to scrap opened year from webpage usign regex for matching\n",
        "\n",
        "6. findTime()\\\n",
        "Function to scrap open and closing time from webpage using regex for matching\n",
        "\n",
        "7. extract_links_with_error_handling()\\\n",
        "Function to crawl and extract links, any other useful data from each link upto 'max_depth' and store it in output file\n",
        "\n",
        "8. crawl_link()\\\n",
        "Function to match strings in links to get various links\n",
        "\n",
        "9. write_output()\\\n",
        "Function to write all info in an output file"
      ],
      "metadata": {
        "id": "xpcVHgBDAK-a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f431ae1a-d68b-4f2a-9f47-6615079a9f6e",
          "showTitle": false,
          "title": ""
        },
        "id": "-Dz2cajNmiEk"
      },
      "outputs": [],
      "source": [
        "def categorize_links(data_dict, links):\n",
        "    if links is None:\n",
        "        return []\n",
        "    other_links = []\n",
        "    for link in links:\n",
        "        if \"menu\" in link:\n",
        "            data_dict[\"restaurant_provided_menu_link\"] = link\n",
        "        elif \"order\" in link:\n",
        "            data_dict[\"order_online_link\"] = link\n",
        "        elif \"facebook\" in link:\n",
        "            data_dict[\"facebook_link\"] = link\n",
        "        elif \"instagram\" in link:\n",
        "            data_dict[\"instagram_link\"] = link\n",
        "        elif \"twitter\" in link:\n",
        "            data_dict[\"twitter_link\"] = link\n",
        "        elif \"play_store\" in link:\n",
        "            data_dict[\"play_store_link\"] = link\n",
        "        elif \"app_store\" in link:\n",
        "            data_dict[\"app_store_link\"] = link\n",
        "        elif \"maps\" in link:\n",
        "            data_dict[\"maps_link\"] = link\n",
        "        else:\n",
        "            other_links.append(link)\n",
        "\n",
        "def findPhone(soup):\n",
        "  phone_pattern = r'\\+\\d{1,2}\\s?\\(\\d{3}\\)\\s?\\d{3}[-\\s]\\d{4}'\n",
        "  phones = []\n",
        "  pattern_elements = soup.find_all(string=re.compile(phone_pattern))\n",
        "  for element in pattern_elements:\n",
        "      stripped_ele = element.strip()\n",
        "      if stripped_ele is not None:\n",
        "          stripped_ele = stripped_ele.strip()\n",
        "      match = re.search(phone_pattern, stripped_ele)\n",
        "      phones.append(match.group())\n",
        "  return phones\n",
        "\n",
        "def findEmail(soup):\n",
        "  email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
        "  pattern_elements = soup.find_all(string=re.compile(email_pattern))\n",
        "  for element in pattern_elements:\n",
        "      stripped_ele = element.strip()\n",
        "      if stripped_ele is not None:\n",
        "          stripped_ele = stripped_ele.strip()\n",
        "      match = re.search(email_pattern, stripped_ele)\n",
        "      return match.group()\n",
        "\n",
        "def findBooleanValues(soup):\n",
        "    keywords_pattern = [\"pickup\", \"group\", \"catering\", \"counter\", \"bar\", \"delivery\", \"gifting\", \"takeout\"]\n",
        "    def findPattern(keyword):\n",
        "        if soup.find_all(string=re.compile(r'\\b(?:' + keyword + r')\\b', re.IGNORECASE)):\n",
        "            return True\n",
        "        return False\n",
        "    dict = {}\n",
        "    for keyword in keywords_pattern:\n",
        "        dict[keyword] = findPattern(keyword)\n",
        "    return dict\n",
        "\n",
        "def findYear(soup):\n",
        "    year_pattern = r'\\b\\d{4}\\b'\n",
        "    pattern_elements = soup.find_all(string=re.compile(year_pattern))\n",
        "    possible_year = []\n",
        "    for element in pattern_elements:\n",
        "        stripped_ele = element.strip()\n",
        "        possible_year.append(stripped_ele)\n",
        "    all_phones = findPhone(soup)\n",
        "    possible_year = [ele for ele in possible_year if not any(phn in ele for phn in all_phones)]\n",
        "    possible_year = [year for year in possible_year if '©' not in year]\n",
        "    actually_possible_years = []\n",
        "    for year in possible_year:\n",
        "      match = re.search(year_pattern, year)\n",
        "      actually_possible_years.append(int(match.group()))\n",
        "    for yr in actually_possible_years:\n",
        "      if(yr > datetime.now().year):\n",
        "        actually_possible_years.remove(yr)\n",
        "    return actually_possible_years\n",
        "\n",
        "def findTime(soup):\n",
        "  time_pattern = r'\\b\\d{1,2}(?::\\d{2})?\\s*[APap][Mm]\\b'\n",
        "  pattern_elements = soup.find_all(string=re.compile(time_pattern, re.IGNORECASE))\n",
        "  for element in pattern_elements:\n",
        "      parent_element = element.find_parent(attrs={\"class\": True})\n",
        "      matches = parent_element.get_text()\n",
        "      formatted_data = ' '.join(matches.split())\n",
        "  return formatted_data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_filename = 'input/input2.csv'\n",
        "output_filename = 'output/output2.csv'\n",
        "\n",
        "link_data_lock = threading.Lock()\n",
        "\n",
        "def write_output(data_dict, output_filename):\n",
        "    header = list(data_dict.keys())\n",
        "    with open(output_filename, 'a', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
        "        if csvfile.tell() == 0:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(data_dict)\n",
        "\n",
        "# Function to crawl a single link and store the result in link_data\n",
        "def crawl_link(link_index, link, max_depth, data_dict):\n",
        "    visited_links = set()\n",
        "    result = extract_links_with_error_handling(\n",
        "        link,\n",
        "        link_index,\n",
        "        data_dict,\n",
        "        visited_links,\n",
        "        depth=0,\n",
        "        max_depth=max_depth\n",
        "    )\n",
        "\n",
        "    categorize_links(data_dict, result)\n",
        "    write_output(data_dict, output_filename)\n",
        "\n",
        "    with link_data_lock:\n",
        "        link_data[link] = result\n",
        "\n",
        "def extract_links_with_error_handling(url, index, data_dict, visited_links=None, depth=0, max_depth=2):\n",
        "    if visited_links is None:\n",
        "        visited_links = set()\n",
        "\n",
        "    # retries = 3  # Number of retries before giving up\n",
        "    # retry_delay = 2  # Delay between retries in seconds\n",
        "    unique_links = set()\n",
        "\n",
        "    try:\n",
        "        if url not in visited_links and depth <= max_depth:\n",
        "            visited_links.add(url)\n",
        "            response = requests.get(url, timeout=(5, 5))\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            for (key, value) in data_dict.items():\n",
        "                if(value == ''):\n",
        "                    if(key ==  'telephone_number'):\n",
        "                        data_dict[key] = findPhone(soup)\n",
        "                    if(key == 'year_established'):\n",
        "                        data_dict[key] = findYear(soup)\n",
        "                    if(key == 'email_address'):\n",
        "                        data_dict[key] = findEmail(soup)\n",
        "                    if( key == 'offers_delivery' or\n",
        "                        key == 'offers_pickup' or\n",
        "                        key == 'offers_group_order' or\n",
        "                        key == 'offers_catering' or\n",
        "                        key == 'has_bar' or\n",
        "                        key == 'has_counter' or\n",
        "                        key == 'has_gifting' or\n",
        "                        key == 'has_takeout' ):\n",
        "                        for(key2, val2) in findBooleanValues(soup).items():\n",
        "                            if(key2 in key and value == ''):\n",
        "                                data_dict[key] = val2\n",
        "\n",
        "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "            valid_links = [link for link in links if re.match(r'^https?://', link)]\n",
        "            for link in valid_links:\n",
        "                if not any(social in link for social in [\"whatsapp\", \"insta\", \"facebook\", \"twitter\"]):\n",
        "                    unique_links.add(link)\n",
        "\n",
        "    except Timeout:\n",
        "        print(f\"Timeout occurred for index {index} and URL {url}. Retrying...\")\n",
        "        # time.sleep(retry_delay)\n",
        "    except RequestException as e:\n",
        "        print(f\"RequestException occurred for index {index} and URL {url}: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting links from index {index} for URL {url}: {str(e)}\")\n",
        "\n",
        "    return unique_links\n",
        "\n",
        "# Initialize an empty dictionary to store the data\n",
        "link_data = {}\n",
        "threads = []\n",
        "\n",
        "with open(input_filename, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    dummy_dict = {}\n",
        "    header_row = next(reader)\n",
        "    for key in header_row:\n",
        "        dummy_dict[key] = None\n",
        "    new_rows = ['instagram_link', 'twitter_link', 'play_store_link', 'app_store_link']\n",
        "    for row in new_rows:\n",
        "        dummy_dict[row] = None\n",
        "\n",
        "    for index, row in enumerate(reader):\n",
        "        if len(row) < 2:\n",
        "            continue\n",
        "        link_index = row[0]\n",
        "        link = row[1]\n",
        "        data_dict = {}\n",
        "        if(link == ''):\n",
        "            continue\n",
        "\n",
        "        for key in dummy_dict.keys():\n",
        "            if key in header_row:\n",
        "                data_dict[key] = row[header_row.index(key)]\n",
        "            else:\n",
        "                data_dict[key] = None\n",
        "\n",
        "        visited_links = set()\n",
        "        max_depth = 1\n",
        "        thread = threading.Thread(target=crawl_link, args=(link_index, link, max_depth, data_dict))\n",
        "        thread.start()\n",
        "        threads.append(thread)\n",
        "        time.sleep(1)\n",
        "\n",
        "# Wait for all threads to finish\n",
        "for thread in threads:\n",
        "    thread.join()"
      ],
      "metadata": {
        "id": "D8ui-sOkJ9jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "823e734d-70e3-4270-88df-2cf86471dac7",
          "showTitle": false,
          "title": ""
        },
        "id": "f0BtXgeumiEk"
      },
      "source": [
        "## Section 4: Data Quality Checks ✅ [5]\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/NS7gPxeumewkWDOIxi/giphy.gif\" width=\"350px\" alt=\"merge\">\n",
        "\n",
        "In this section, you are expected to devise and execute a series of data quality tests on the dataframe. Your objective is to identify and address potential edge cases that could affect the downstream use of the dataset. You may use [Great Expectations](https://greatexpectations.io/) or any other tool of your preference to run the tests and profile the data.\n",
        "\n",
        "\n",
        "- **Test Design**: Outline the data quality tests you plan to run. Explain why you have chosen these tests and what potential issues they could uncover.\n",
        "- **Test Execution**: Execute the designed tests on the dataframe. Document the results of each test, including any discrepancies or anomalies identified.\n",
        "- **Data Profiling**: Perform a thorough profiling of the data to understand its characteristics and quality. This may include understanding the distribution of different features, identifying outliers, or assessing the completeness and uniqueness of the data.\n",
        "- **Edge Case Identification**: Discuss any edge cases you have identified during the testing and profiling process. Explain how these edge cases could affect the downstream use of the dataset and propose potential solutions to address them.\n",
        "\n",
        "> 📝 Your ability to design comprehensive tests, identify and address edge cases, and thoroughly profile the data will be crucial in this section. Be sure to document your process and findings clearly and concisely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e7820a32-6392-4d72-ad17-fcc4f3ac54de",
          "showTitle": false,
          "title": ""
        },
        "id": "9IB_2p8bmiEl"
      },
      "outputs": [],
      "source": [
        "merged_df.to_csv(\"testing.csv\")\n",
        "import great_expectations as ge\n",
        "df_1=ge.read_csv(\"testing.csv\")\n",
        "df_1.expect_column_to_exist(\"rating\")\n",
        "df_1.expect_column_distinct_values_to_be_in_set('mapped_range',['$11-30','$31-60','Above $61'])\n",
        "df_1.expect_column_values_to_be_in_set('is_closed',value_set=[True,False])\n",
        "df_1.expect_column_values_to_match_regex('year_established',r'\\b\\d{4}\\b')\n",
        "df_1.expect_column_unique_value_count_to_be_between('year_established',min_value=1900,max_value=2023)\n",
        "df_1.expect_column_values_to_be_between('rating',max_value= 10,min_value=1)\n",
        "df_1.expect_column_values_to_match_regex('operating_hours_sun',r'\\d{1,2}:\\d{2} [APap][Mm] - \\d{1,2}:\\d{2} [APap][Mm](?: , \\d{1,2}:\\d{2} [APap][Mm] - \\d{1,2}:\\d{2} [APap][Mm])*')\n",
        "df_1.expect_column_values_to_match_regex('operating_hours_sat',r'\\d{1,2}:\\d{2} [APap][Mm] - \\d{1,2}:\\d{2} [APap][Mm](?: , \\d{1,2}:\\d{2} [APap][Mm] - \\d{1,2}:\\d{2} [APap][Mm])*')\n",
        "df_1.expect_column_values_to_be_in_set(\n",
        "    column=\"is_claimed\",  # The name of the column you want to validate\n",
        "    value_set=[True, False],  # List of expected valid values\n",
        "    mostly=True\n",
        ")df_1.expect_column_values_to_match_regex(\"website_url\",r'^(https?://)[^\\s/$.?#].[^\\s]*$')\n",
        "df_1.expect_column_values_to_match_regex(\"facebook_link\",r'^(https?://)?(www\\.)?facebook\\.com/[A-Za-z0-9_.-]+/?$')\n",
        "df_1.expect_column_values_to_match_regex(\"telephone_number\",r'^\\(\\d{3}\\) \\d{3}-\\d{4}$')\n",
        "df_1.expect_column_values_to_match_regex(\"restaurant_provided_menu_link\",r'^(https?://)?(www\\.)?([a-zA-Z0-9-]+)(\\.[a-z]{2,4})(/[a-zA-Z0-9-_.~%]*)*([?][a-zA-Z0-9-_.~%&=]*)?$')\n",
        "df_1.expect_column_values_to_be_of_type('address',\"object\")\n",
        "df_1.expect_column_value_lengths_to_be_between('subregion',2,3)\n",
        "df_1.expect_column_mean_to_be_between('latitude',34,38)\n",
        "df_1.expect_column_mean_to_be_between('longitude',-119,-122)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "80efd06e-1048-40f1-a01c-3c09f3b5ea73",
          "showTitle": false,
          "title": ""
        },
        "id": "jUm20zpWmiEl"
      },
      "source": [
        "Save your results in a parquet file and upload them to your private fork of the GitHub Repository."
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "odyssey",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}